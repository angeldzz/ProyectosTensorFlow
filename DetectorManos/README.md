# ğŸ¤– Detector de Gestos con Inteligencia Artificial

Un sistema completo de reconocimiento de gestos de mano en tiempo real utilizando MediaPipe, TensorFlow y OpenCV. Este proyecto permite detectar y clasificar diferentes gestos como piedra, papel, tijera, lagarto, bien y mal a travÃ©s de la cÃ¡mara web.

## ğŸŒŸ CaracterÃ­sticas

- **DetecciÃ³n en tiempo real**: Reconocimiento instantÃ¡neo de gestos usando la cÃ¡mara web
- **6 gestos soportados**: Piedra, papel, tijera, lagarto, bien y mal
- **Alta precisiÃ³n**: Modelo de red neuronal entrenado con mÃºltiples ejemplos
- **Interfaz visual**: VisualizaciÃ³n de landmarks de manos y resultados en pantalla
- **FÃ¡cil extensiÃ³n**: Sistema modular para agregar nuevos gestos

## ğŸ“‹ Requisitos del Sistema

- Python 3.8 o superior
- CÃ¡mara web funcional
- Sistema operativo: Windows, macOS o Linux

## ğŸ› ï¸ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <url-del-repositorio>
cd DetectorManos
```

### 2. Crear entorno virtual (recomendado)
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

### Dependencias principales
- **TensorFlow 2.19.0**: Framework de aprendizaje automÃ¡tico
- **MediaPipe 0.10.14**: DetecciÃ³n de manos y landmarks
- **OpenCV 4.12.0.88**: Procesamiento de imÃ¡genes y video
- **NumPy 2.1.3**: ManipulaciÃ³n de arrays numÃ©ricos
- **Scikit-learn 1.7.1**: Herramientas de machine learning

## ğŸš€ Uso del Sistema

### Ejecutar el detector
```bash
python DetectorGestosIA.py
```

### Controles
- **Muestra tu mano** frente a la cÃ¡mara para ver el gesto detectado
- **Presiona 'q'** para salir del programa

## ğŸ“Š Entrenar el Modelo

### 1. Recolectar datos de gestos
```bash
python recolectorDeGestos.py
```

**Instrucciones para recolecciÃ³n:**
1. Modifica la variable `GESTO` en el archivo con el nombre del gesto a capturar
2. Ejecuta el script y posiciona tu mano frente a la cÃ¡mara
3. Presiona **'s'** para guardar ejemplos del gesto (recomendado: 100-200 ejemplos)
4. Presiona **'q'** para finalizar la recolecciÃ³n
5. Repite el proceso para cada gesto

### 2. Entrenar el modelo
```bash
python entrenar_modelo.py
```

El entrenamiento generarÃ¡ el archivo `gestos_modelo_v2.keras` con el modelo entrenado.

## ğŸ“ Estructura del Proyecto

```
DetectorManos/
â”œâ”€â”€ DetectorGestosIA.py          # Detector principal en tiempo real
â”œâ”€â”€ entrenar_modelo.py           # Script de entrenamiento del modelo
â”œâ”€â”€ recolectorDeGestos.py        # Recolector de datos de gestos
â”œâ”€â”€ gestos_modelo_v2.keras       # Modelo entrenado (generado)
â”œâ”€â”€ datos/                       # Carpeta con datos de entrenamiento
â”‚   â”œâ”€â”€ papel.csv
â”‚   â”œâ”€â”€ tijera.csv
â”‚   â”œâ”€â”€ piedra.csv
â”‚   â”œâ”€â”€ lagarto.csv
â”‚   â”œâ”€â”€ bien.csv
â”‚   â””â”€â”€ mal.csv
â””â”€â”€ requirements.txt             # Dependencias del proyecto
```

## ğŸ”§ ConfiguraciÃ³n del Modelo

### Arquitectura de la Red Neuronal
- **Capa de entrada**: 63 caracterÃ­sticas (21 landmarks Ã— 3 coordenadas)
- **Capa oculta 1**: 256 neuronas + ReLU + Dropout (0.4)
- **Capa oculta 2**: 128 neuronas + ReLU + Dropout (0.4)
- **Capa de salida**: 6 neuronas + Softmax

### ParÃ¡metros de entrenamiento
- **Optimizador**: Adam (learning rate: 0.0001)
- **FunciÃ³n de pÃ©rdida**: Sparse Categorical Crossentropy
- **Ã‰pocas**: 2000
- **Batch size**: 32
- **ValidaciÃ³n**: 20% de los datos

## ğŸ¯ Gestos Soportados

| Gesto | DescripciÃ³n |
|-------|-------------|
| ğŸ‘‹ Papel | Mano abierta con dedos extendidos |
| âœŒï¸ Tijera | Dedos Ã­ndice y medio extendidos |
| âœŠ Piedra | PuÃ±o cerrado |
| ğŸ¦ Lagarto | Gesto tipo "lagarto" de piedra-papel-tijera-lagarto-spock |
| ğŸ‘ Bien | Pulgar hacia arriba |
| ğŸ‘ Mal | Pulgar hacia abajo |

## ğŸ”„ PersonalizaciÃ³n

### Agregar nuevos gestos
1. Modifica la lista `gestos` en todos los archivos Python
2. Recolecta datos del nuevo gesto usando `recolectorDeGestos.py`
3. Reentrena el modelo con `entrenar_modelo.py`

### Ajustar precisiÃ³n
- Aumenta el nÃºmero de ejemplos por gesto (recomendado: 200-500)
- Modifica los parÃ¡metros del modelo en `entrenar_modelo.py`
- Ajusta el learning rate o aÃ±ade mÃ¡s capas

## ğŸ› SoluciÃ³n de Problemas

### El modelo no detecta correctamente
- **SoluciÃ³n**: Recolecta mÃ¡s datos de entrenamiento
- **Consejo**: AsegÃºrate de tener buena iluminaciÃ³n y fondo contrastante

### Error de cÃ¡mara
- **SoluciÃ³n**: Verifica que la cÃ¡mara no estÃ© siendo usada por otra aplicaciÃ³n
- **Consejo**: Cambia el Ã­ndice de cÃ¡mara en `cv2.VideoCapture(0)` a `1` o `2`

### Rendimiento lento
- **SoluciÃ³n**: Reduce la resoluciÃ³n de la cÃ¡mara o el nÃºmero de Ã©pocas de entrenamiento
- **Consejo**: Usa una GPU compatible con TensorFlow para acelerar el entrenamiento

## ğŸ“ˆ MÃ©tricas de Rendimiento

El modelo actual alcanza aproximadamente:
- **PrecisiÃ³n de entrenamiento**: ~95%
- **PrecisiÃ³n de validaciÃ³n**: ~90%
- **FPS en detecciÃ³n**: 15-30 (dependiendo del hardware)

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Si tienes ideas para mejoras:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado con â¤ï¸ para la comunidad de IA y visiÃ³n por computadora.

---

### ğŸ“ Soporte

Si tienes preguntas o necesitas ayuda:
- Abre un issue en el repositorio
- Revisa la documentaciÃ³n de las bibliotecas utilizadas
- Consulta los ejemplos en el cÃ³digo

**Â¡DiviÃ©rtete detectando gestos! ğŸ‰**